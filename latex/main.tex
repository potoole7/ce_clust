\documentclass{article}

\usepackage[top=3cm, bottom=3cm, left=3.5cm,right=3.5cm]{geometry}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{titlesec}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{natbib} % print author's name and year when citing
\usepackage{bbm}
\usepackage{todonotes}
\usepackage{pdflscape}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{pdfpages}
\usepackage{setspace} 
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{tikz}
\usepackage[colorlinks=true,citecolor=blue, linkcolor=blue]{hyperref}
\usepackage{multirow}
\usepackage{todonotes}

\usepackage{tikz}

\setlength{\tabcolsep}{5pt}
%%\setlength{\parindent}{0pt}
\usepackage[parfill]{parskip}
\renewcommand{\arraystretch}{1.5}

\setcounter{tocdepth}{2}

% Define a custom note command for general notes
\newcommand{\mynote}[1]{\todo[color=yellow!40,inline]{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}

% Nature Bibliography style
% \usepackage[backend=biber,style=nature]{biblatex}
% \addbibresource{library.bib} 
\bibliographystyle{unsrtnat}

% number equations by section
\numberwithin{equation}{section}

\title{A clustering framework for conditional extremes models}
\thispagestyle{empty}
\author{Patrick O'Toole, Christian Rohrbeck, Jordan Richards}
\date{\today}

\begin{document}

\maketitle

\todo{Same (present), tense throughout if possible}

% \newpage

\begin{abstract}
  % Intro
 Conditional extreme value models have proven useful for analysing the joint tail behaviour of random vectors. 
 Conditional extreme value models describe the distribution of components of a random vector conditional on at least one exceeding a suitably high threshold, and they can flexibly capture a variety of structures in the distribution tails.
 One drawback of these methods is that model estimates tend to be highly uncertain due to the natural scarcity of extreme data. 
 This motivates the development of clustering methods for this class of models; pooling similar within-cluster data drastically reduces parameter estimation uncertainty.
 
 While an extensive amount of work to estimate conditional extremes models exists in multivariate and spatial applications, the prospect of clustering for models of this type has not yet been explored. 
 As a motivating example, we explore tail dependence of meteorological variables across multiple spatial locations and seek to identify sites which exhibit similar multivariate tail behaviour. 
 To this end, we introduce a clustering framework for conditional extremes models which provides a novel and principled, parametric methodology for summarising multivariate extremal dependence.
 
 % Outline of model
 In a first step, we define a dissimilarity measure for conditional extremes models based on the Jensen-Shannon divergence and common working assumptions made when fitting these models. 
 One key advantage of our measure is that it can be applied in arbitrary dimension and, as opposed to existing methods for clustering extremal dependence, is not restricted to a bivariate setting. 
 Clustering is then performed by applying the k-medoids algorithm to our novel dissimilarity matrix, which collects the dissimilarity between all pairs of spatial sites. 
 
 % Simulation
 A detailed simulation study shows our technique to be superior to the leading competitor in the bivariate case across a range of possible dependence structures and uniquely provides a tool for clustering in the multivariate extremal dependence setting. 
 We also outline a methodology for selecting the number of clusters to use in a given application.  Finally, we apply our clustering framework to meteorological data from Ireland and air pollution data in cities across the US (United States). 

\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}\label{sec:intro}
\todo{Where to describe Vignotto method? in introduction?}
\todo{Will have to mention that other paper that does bivaraite clustering as well in background, right?}

\section{Motivating examples}
\subsection{Irish meteorological data} \label{subsec:mot_irl}
\todo{Copy mostly from TFR, will have to include NI data}

\begin{itemize}
    \item 
\end{itemize}

\subsection{US urban air pollution} \label{subsec:mot_us}
\todo{See Huser paper for inspiration}

\begin{itemize}
    \item 
\end{itemize}

\section{Methods}
\subsection{Peaks-over-threshold}

% Christian said not needed! What exactly should we have instead??

\subsection{Extremal dependence}

\todo{Coppied straight from TFR, may have to cut down}

In order to characterise the dependence between extremes, we must introduce the concepts of asymptotic dependence and independence, as described in \citet{Coles1999}.
% The coefficient of asymptotic dependence, $\chi \in [0, 1]$, measures the degree of asymptotic dependence between two random variables $X_1$ and $X_2$ with corresponding CDFs $F_1$ and $F_2$, under the class of asymptotically dependent distributions.
The coefficient of dependence, $\chi \in [0, 1]$, measures the degree of extremal dependence between two random variables $X_1$ and $X_2$ with corresponding CDFs $F_1$ and $F_2$, under the class of asymptotically dependent distributions. \todo{expand CDF?}
It is defined as $\chi = \lim_{u \rightarrow 1}{\chi(u)}$, where
% \[
%   \chi(u) = \mathbb{P}\left( F_1(X_1) > u \mid F_2(X_2) > u \right), u \in [0, 1].
% \]
\[
\chi(u) = 2 - \frac{\log\left(\mathbb{P}(F_1(X_1) < u, F_2(X_2) < u)\right)}{\log \left(\mathbb{P}(F_1(X_1) < u)\right)}.
\]

If $\chi = 0$, then $X_1$ and $X_2$ are said to be asymptotically independent, while for $\chi > 0$, they are asymptotically dependent, with greater magnitude as $\chi$ increases.
Thus, $\chi$ gives the limit probability of $X_1$ being extreme given that $X_2$ is extreme, and we expect the joint probability of observing extremal events in both variables to be greater where $\chi$ is higher \citep{Rohrbeck2021}.
Furthermore, $\chi$ is symmetric in $X_1$ and $X_2$.
Two variables can be asymptotically dependent even if they are not dependent in the classical sense.
% Another coefficient, $\bar{\chi} \in [-1, 1]$, is used to measure the strength of dependence in the case of asymptotically independent distributions, where $\chi = 0$.
Another coefficient, $\bar{\chi} \in [-1, 1]$, measures the tail dependence between two variables where $\chi = 0$, in the case of asymptotically independent variables.
This coefficient is similarly defined as $\bar{\chi} = \lim_{u \rightarrow 1}{\bar{\chi}(u)}$, where
\[
  \bar{\chi}(u) = \lim_{u \rightarrow 1} \frac{2\log\mathbb{P}(F_1(X_1) > u)}{ \log{\mathbb{P}(F_1(X_1) > u, F_2(X_2) > u)}} - 1.
\]
For asymptotic dependence, $\bar{\chi} = 1$.
Under asymptotic independence, $\bar{\chi} = 0$, with $\bar{\chi}$ increasing in magnitude as the variables become more positively (for $\bar{\chi} > 0$) or negatively (for $\bar{\chi} < 0$) dependent in their extremes \citep{Vignotto2021}.

Together, $(\chi, \bar{\chi})$ provide a useful summary of extremal dependence for any pair of random variables.
However, they do not fully describe the dependence structure of a multivariate distribution.
For example, probabilities of jointly extreme events for two or more variables cannot be calculated from these coefficients.
For this, we require a model for extremal dependence.
Various approaches to this problem have been proposed, including max-stable processes, Pareto processes, Gaussian processes and copulas.
However, these models are limited in their ability to model dependence and their computational feasibility, and are often conceptually difficult. 
% For example, max-stable processes can only model $\mathbb{P}(\bm{X} \in \bm{C})$ under asymptotic dependence between each pair of variables in $\bm{X}$, while in contrast Gaussian processes can only model asymptotic independence \citep{Tawn2018, Huser2024}.
% Max-stable processes are also quite computationally expensive and conceptually difficult to derive, and so are often not feasible for large datasets.
A framework which provides a more flexible approach to modelling extremal dependence is the CE model of \citet{Heffernan2004}.

\subsection{Conditional extremes}

% See Jordan's paper for quick summary (TFR a bit long winded)
% Need to talk about marginals, transformations and then model itself

\subsection{Jensen-Shannon divergence}

% Define KL divergence, show closed form for two normals, then JS divergence

% Intro: CE gives distributions, which need to be compared
\begin{itemize}
  % \item \citet{Heffernan2004} shows that inference can be made about the extremal dependence structure of a multivariate distribution by conditioning on the exceedance of a high threshold by at least one of the variables, using that 
 \item \citet{Heffernan2004} shows that inference can be made about the extremal dependence of a variable $Y_j$ on another variable $Y_i$ by conditioning on the exceedance of a high threshold by $Y_i$, using that
    \begin{equation} \label{eq:ce_norm}
      % \bm{Y}_{-i} \mid \bm{Y}_i = y_i \sim N\left(\bm{\alpha}_{\mid i} y_i + y_i^{\bm{\beta}_i} \bm{\mu}_{\mid i}, y_i^{\bm{\beta}_i} \bm{\sigma_{\mid i}}\right), 
      Y_{j} \mid Y_{i} = y_i \sim N\left(\alpha_{j \mid i} y_i + y_i^{\beta_i} \mu_{j \mid i}, y_i^{\beta_i} \sigma_{j \mid i}\right),
    \end{equation}
for $Y_i > u$.
  \item Therefore, when hoping to compare the extremal dependence structure of two locations, a sensible approach might be to compare the conditional distributions given by equation \ref{eq:ce_norm} for each variable. 
\end{itemize}

\vspace{1cm}

% KL Divergence
\begin{itemize}
  \item One common measure of dissimilarity between two distributions $P$ and $Q$ is the Kullback-Leibler (KL) divergence, defined as
    \begin{equation} \label{eq:kl}
      D_{KL}(P \mid\mid Q) = \int_{-\infty}^{\infty} p(y) \log\left(\frac{p(y)}{q(y)}\right) dy.
    \end{equation}
    where $p(y)$ and $q(y)$ are the densities of $P$ and $Q$, respectively \citet{Kullback1951}.
  \item The dissimilarity of two distributions can be measured using the KL divergence, which is always non-negative and zero if and only if $P = Q$.
  \item In this case, $P$ and $Q$ are the conditional distribution given by equation \ref{eq:ce_norm} for any two locations.
  % \item It can be shown that if $P$ and $Q$, which in this case will specify the conditional distributions in equation \ref{eq:kl} for two locations, are both normal, then the KL divergence has a closed form solution.
  \item Generally speaking, if we define $P$ and $Q$ as being normal distributions with means $\mu_1$ and $\mu_2$ and standard deviations $\sigma_1$ and $\sigma_2$, then the KL divergence has a closed form solution which is given by
    \begin{equation} \label{eq:kl_norm}
      D_{KL}(P \mid\mid Q) = \frac{1}{2} \left( \frac{(\mu_2 - \mu_1)^2}{\sigma_2^2} + \frac{\sigma_1^2}{\sigma_2^2} - \log{(\frac{\sigma_1^2}{\sigma_2^2})} - 1 \right),
    \end{equation}
    as shown in \citet{Soch2020_norm_kl}.
  % \item If we say that $Y_j \mid \Y_i \sim P$ and $Y_j \mid Y_i \sim Q$ for two locations, then we can calculate the KL divergence between these two conditional distributions by plugging in means and standard deviations of the normal distributions into 
  \item We can then calculate the KL divergence between the conditional distributions given by equation \ref{eq:ce_norm} for two locations by plugging in the means and standard deviations of the normal distributions into equation \ref{eq:kl_norm}. \todo{Do I need to actually show this calculation? Easy enough}
  \item This closed form solution means that the JS divergence can be easily and efficiently calculated.
  \item One question to answer is the values to use for $y_i$ in equation \ref{eq:ce_norm}, which should be the same for both locations, to assess whether the extremal dependence structure at the same conditioning level is similar.
  \item One somewhat arbitrary approach is to take the maximum threshold between the two CE fits, and then use 10 points between this threshold and twice its value, calculating the JS divergence between these points.
\end{itemize}

\vspace{1cm}

% JS Divergence
\begin{itemize}
  \item One limitation of the KL divergence is that it is not symmetric, making it a poor choice as a dissimilarity measure for clustering. 
  \item The Jensen-Shannon (JS) divergence is a simple extension of the JS divergence which provides a symmetric measure of dissimilarity between two distributions, defined as \todo{reference JS divergence}
    \begin{equation} \label{eq:js}
      D_{JS}(P \mid\mid Q) = \frac{1}{2} D_{KL}(P \mid\mid M) + \frac{1}{2} D_{KL}(Q \mid\mid M),
    \end{equation}
    where $M = \frac{1}{2}(P + Q)$ \citep{Lin1991}.
  \item Since we have a closed form for the KL divergence when both $P$ and $Q$ are normal, we also have a closed form for the JS divergence.
  \item Hence, we have a simple and interpretable measure of dissimilarity between two conditional distributions given by equation \ref{eq:ce_norm} for two locations, with which we can cluster locations based on their extremal dependence structure.
  \item This measure can be applied in arbitrary dimension, by simply summing the JS divergence across all variables, and is not restricted to a bivariate setting, as is \cite{Vignotto2021}.
\end{itemize}

\subsection{Clustering}

% Describe clustering process via JS divergence and k-medoids/PAM algorithm (give?
% See Vignotto)

% PAM (K-means not suitable as clusters cannot be assumed to be spherical)
\begin{itemize}
\end{itemize}

\subsubsection{Choice of k}

% Describe methods for choosing k (AIC, TWGSS) (silhouette not suitable)

\section{Simulation study}\label{sec:sim}
\todo{May not be subsections, but will form structure of this section}
\todo{Do I want to add any visualisations of LRI here? Any ideas there?}
\todo{Do I need to go into specifics about numbers in reduction of variation etc? Or are the plots self-evident?}
\todo{May want tables instead of figures for some/all simulation results}
\todo{How do I get the rho in ggplot to match the font used in LaTeX? and not clip?}
\todo{Should I smooth across points in sum plots with geom smooth? Or continue to use 90\% credible interval (which would improve for 500 simulations vs 200 I've provisionally used}
\todo{Redo simulations while not estimating marginal GPDs (already known!)}

% Introduce section
\begin{itemize}
  \item Throughout this section, we evaluate the effectiveness of our clustering method using a simulation study.
  \item In \ref{subsec:sim_gauss}, a simple simulation design using a Gaussian copula to generate data is described. 
  This data has the advantage of having theoretical asymptotic values for the CE parameters, to which we can directly compare our finite simulation estimates, as a test to ensure that our clustering is working as desired, reducing uncertainty in parameter estimates. 
  \item In \ref{subsec:sim_mixture}, we extend this study to a more complex simulation design using a mixture of Gaussian and t-copulas, for a variety of simple and more complex dependence structures and scenarios. 
  \item We evaluate our clustering solution using the ARI, and in \ref{subsubsec:boot} ascertain the improvement of parameter estimation post-clustering using a bootstrapping scheme. 
\end{itemize}

\subsection{Gaussian copula} \label{subsec:sim_gauss}

% Single location
Our Gaussian copula simulation design for a single ``location'', nomenclature chosen to match the spatial nature of our applications, is as follows:
\begin{itemize}
  \item We generate multivariate data from a bivariate Gaussian copula, where we control the dependence structure through its correlation parameter, $\rho_{N}$. 
    \cite{Keef2013} show that for a bivariate Gaussian copula, the asymptotic CE parameters are $\alpha = \text{sign}(\rho_{\text{Gauss}}) \rho_{\text{Gauss}}^2, \beta = 1/2$, indicating asymptotic independence. \todo{Fix citations}
  \item We sample from this copula and transform to GPD margins with shape $\xi = -0.05$, scale $\sigma = 1$, to generate our data. \todo{mention why we chose these?}
  \item Each variable in this multivariate dataset may represent, for example, a meteorological or air pollution variable at a given location.
  \item We can then transform to Laplace margins and use the CE model to estimate the multivariate extremal dependence structure at this location.
\end{itemize}

\vspace{1cm}

% Multiple locations
\begin{itemize}
  \item We can easily extend this design by generating multiple locations with the same and different $\rho_{\text{Gauss}}$ values.
  \item The knowledge of these parameters provides our ``known'' clustering solution.
  \item For example, we could generate data for four locations, the first cluster being two locations with $\rho_{\text{Gauss}} = 0.1$, and the second having $\rho_{\text{Gauss}} = 0.9$ for its two locations.
  \item We can use the CE model to estimate the multivariate extremal dependence structure at each location, and then apply the JS divergence to cluster these locations based on the similarity of the estimated CE parameters. 
  \item These simulations can be repeated many times and we can evaluate the clustering solution using the ARI, and evaluate the bias and variance of our estimates around their theoretical values.
\end{itemize}

% results
\begin{itemize}
  \item To this end, we generated data for 12 locations each with 1000 observations of two variables using the above design.
  \item Three known clusters were designated by having $\rho_{\text{Gauss}}$ values of 0.1, 0.5, and 0.9 respectively for four locations each.
  \item Conditional dependence quantiles of 0.9 and 0.99 were used to estimate the CE model parameters before and after clustering. 
  \item The number of clusters $k$ was ascertained using the method described in ? \todo{reference part of methods that describes this}
  \item Simulations were repeated 500 times, and the results are shown in figure \ref{fig:00_gauss_cop}.
  \item The estimation of $k$ and the ARI for this simple, well-defined example were perfect across all simulations. \todo{double check this assertion!}
  \item Several interesting points can be made from the results.
  \item Pre-clustering, the CE model estimates have greater uncertainty, especially for $\beta$, where the bias is also larger.
  \item This is especially the case for the higher dependence quantile, where data is scarcer, although this also corresponds to less bias, with the true values often lying within the first and third quantiles of each boxplot, as we take our data from further into the tail, where convergence to the asymptotic values is better.
  \item Bias and variance are lower for higher $\rho_{\text{Gauss}}$, which appears to also be associated with better convergence to the theoretical asymptotic values.
  \item Post-clustering, the same patterns emerge, but the variability in the estimates is reduced, especially in the problematic cases of estimating $\beta$ and for the higher conditional quantile level, as desired. \todo{Quote any specific values? Or are plots self-evident?}
\end{itemize}

\todo{Stop alpha and beta facet labels from clipping}
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\linewidth]{plots/sim_01_gauss_cop.png}
    \caption{Boxplots of \emph{$\alpha$ and $\beta$ parameter estimates for the conditional extremes model pre- and post-clustering for a bivariate Gaussian copula simulation of 12 datasets, or ``locations'', each with 1000 observations of each variable, belonging equally to three known clusters. Simulations were repeated 500 times. The x-axis represents the conditional quantile used to fit the conditional extremes model, and the boxes are coloured by their Gaussian copula correlation parameter, $\rho_{\text{Gauss}}$. Dotted horizontal lines indicate the theoretical asymptotic values of $\alpha$ and $\beta$ for a bivariate Gaussian copula, coloured by the same $\rho_{\text{Gauss}}$ values for $\alpha$ and black for $\beta$ to indicate the same value across all clusters, at $1/2$.}}
    \label{fig:00_gauss_cop}
\end{figure}


\subsection{Mixture models} \label{subsec:sim_mixture}

\begin{itemize}
  \item In this section, we extend our simulation design to a mixture of Gaussian and t-copulas, where we control the dependence structure through their respective correlation parameters, $\rho_{\text{Gauss}}$ and $\rho_{t}$, with each t-copula having 3 degrees of freedom.
  \item The idea behind this design is that the Gaussian copula generates observations exhibiting extremal independence, whereas the t-copula induces extremal dependence, the strength of which is determined by their respective correlation parameters.
  \item While there are no theoretical guarantees about our estimates, we can still evaluate our clustering solution for this more difficult setting using the ARI.
  \item Below, we will describe the results of this simulation study in a variety of scenarios, as defined by a grid search over possible $\rho_{\text{Gauss}}$ and $\rho_{t}$ values. 
  \item In \ref{subsubsec:sim_competing_methods}, we compare our method to the leading competitor in the bivariate case, \cite{Vignotto2021}, and also show how our clustering algorithm uniquely provides a tool for clustering in the multivariate extremal dependence setting in \ref{subsubsec:sim_extension}.
  \item A more realistic setting is also considered in \ref{subsubsec:sim_realistic}, where we simulate data from 60 locations with three unequally sized clusters, and perturb the correlation parameters to make the clustering solution more challenging.
  \item We can also evaluate the improvement in parameter estimation post-clustering using the bootstrapping scheme described in \cite{Heffernan2004}, as done in \ref{subsubsec:boot}.
\end{itemize}

\subsubsection{Comparison to competing methods} \label{subsubsec:sim_competing_methods}

\todo{Extend to 500 simulations, only ran 200 times\!}
\begin{itemize}
  \item We compared our method to the leading competitor in the bivariate case, \cite{Vignotto2021}, using the maximum risk function.
  \item As in \ref{subsec:sim_gauss}, we generated data for 12 locations each with 1000 observations of two variables, but this time using a mixture of Gaussian and t-copulas with GPD margins. 
  \item Two known clusters were defined using different values of $\rho_t$, keeping $\rho_{\text{Gauss}}$ the same for both clusters (but varying across simulations).
  \item The CE model was fit at the 90th dependence quantile. 
  \item The clustering solutions were evaluated using the ARI, with results shown in figure \ref{fig:01_ce_vs_vi} for a grid search over different $\rho_{\text{Gauss}}$ and $\rho_{t}$ values, with 500 simulations for each point.
  \item The proposed method is shown to be superior to the method of \cite{Vignotto2021}, across a range of possible bivariate dependence structures, as defined by $\rho_t$.
  \item Both models perform better when the difference in the t-copula correlation parameters between the two clusters is larger, as the clusters are naturally more distinct when there is greater disparity in dependence.
  \item Both models also perform better when Gaussian and t-copula correlation are higher, as the signal of extremal dependence is stronger, with more extremal data available.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\linewidth]{plots/sim_01b_ce_vs_vi_dqu_0.9.png}
    \caption{\emph{Comparison of clustering methods for two variables and two equally sized clusters for simulations of 12 ``locations'', each with 1000 observations of each variable, from a mixture of Normal and Gaussian copulas. A grid search was performed,  with the x-axis representing the Gaussian correlation parameter used for both clusters, and the facet labels showing the t-copula correlation parameters for each ``known'' cluster. This grid search was repeated 500 times. The lines show the median of the Adjusted Rand Index for both clustering methods, with uncertainty coming from the 90\% credible interval.}} % The proposed method is shown to be superior to the leading competitor in the bivariate case, that of \cite{Vignotto2021}. Both models perform better when the difference in the t-copula correlation parameters between the two clusters is larger, and when Gaussian correlation is higher.}}
    \label{fig:01_ce_vs_vi}
\end{figure}


\subsubsection{Extension to $>2$ dimensions} \label{subsubsec:sim_extension}

% \begin{itemize}
%     \item Also shown to work well for three dimensions ...
% \end{itemize}

\begin{itemize}
  \item While the method in \cite{Vignotto2021} is restricted to two dimensions, our method can be extended to three dimensions and beyond.
  \item To illustrate this, we can repeat the simulation study and grid search of \ref{subsubsec:sim_competing_methods}, but now for three variables, the results of which are shown in figure \ref{fig:02_3d}.
  \item The proposed method is shown to work well for three dimensions, and the same patterns in the ARI emerge as in the bivariate case.
  \item The method is shown to actually perform better in three dimensions than in two.
  \item This comes with the caveat that in these simulations, the variables all have the same GPD margins, and so including three variables naturally increases the amount of data available to estimate the same dependence structure as in the bivariate case, and does not introduce additional noise which might be present in real multivariate applications.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\linewidth]{plots/sim_01c_js_sens_3_var_dqu_0.9.png}
    \caption{\emph{Evaluation of clustering performance for three variables and two equally sized clusters for simulations of 12 ``locations'', each with 1000 observations for each variable, from a mixture of Normal and Gaussian copulas. A grid search was performed,  with the x-axis representing the Gaussian correlation parameter used for both clusters, and the facet labels showing the t-copula correlation parameters for each ``known'' cluster. This grid search was repeated 500 times. The lines show the median of the Adjusted Rand Index for both clustering methods, with uncertainty coming from the 90\% credible interval. Points show individual values of the ARI for a given simulation.}} 
    \label{fig:02_3d}
\end{figure}

\subsubsection{More realistic example} \label{subsubsec:sim_realistic}

% \begin{itemize}
%     \item Generated more realistic example to somewhat match the structure of the Irish dataset. 
%     \item ? locations, ... \todo{fill in}
% \end{itemize}

\begin{itemize}
  \item We also design a more realistic example to somewhat mimic the structure of the Irish dataset introduced in \ref{subsec:mot_irl}.
  \item We generated data from 60 locations, each with 1000 observations of two variables, using a mixture of Gaussian and t-copulas with GPD margins.
  \item The 60 locations had three known clusters, with 10, 20, and 30 locations in each cluster, respectively.
  \item A constant $\rho_{\text{Gauss}}$ of 0.5 was used for all locations and simulations, to reduce the dimensionality of the grid search.
  \item A slight perturbation via a uniform sample between $-0.05$ and $0.05$ was added to the correlation parameters of the t-copula for each location, to make the clustering solution more challenging.
  \item Again, clustering was done on the CE model parameters using the JS divergence, and the ARI for a grid search over different $\rho_t$ values is shown in figure \ref{fig:03_realistic}.
  \item The algorithm is again shown to perform well, particularly where the difference in the t-copula correlation parameters between the clusters is largest, as expected.
  \item The variability in the ARI across simulations is also quite low, indicating that the clustering solution is robust.
\end{itemize}

\todo{Check facet and x-axis labels}
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\linewidth]{plots/sim_01d_js_sens_3_var_dqu_0.9.png}
    \caption{\emph{Evaluation of clustering performance for two variables and three clusters for simulations from a mixture of Normal and Gaussian copulas in a more realistic clustering setting. 60 locations with 1000 observations for each variable were simulated, with 10, 20 and 30 belonging to each respective cluster. A grid search was performed, with the x-axis and facet labels showing the t-copula correlation parameters for each of the three ``known'' clusters, for a Gaussian copula correlation of 0.5. Perturbations were added to these parameters. This grid search was repeated 500 times. The line shows the median of the Adjusted Rand Index for both clustering methods, with uncertainty coming from the 90\% credible interval. Points show individual values of the ARI for a given simulation.}} 
    \label{fig:03_realistic}
\end{figure}

\subsubsection{Parameter estimation pre- and post-clustering} \label{subsubsec:boot}

% \begin{itemize}
%     \item Desire to ascertain whether dependence parameters are less uncertain after clustering. 
%     \item Can bootstrap using scheme in \cite{Heffernan2004} to determine uncertainty in parameter estimates. 
%     \item Post clustering, can see that uncertainty is vastly reduced for both $\alpha$ and $\beta$ parameters in this simulation study. 
% \end{itemize}

% Describe bootstrapping
\begin{itemize}
  \item Finally, we recall that an important reason for spatial clustering is to reduce the uncertainty in parameter estimates while minimising the introduction of bias by grouping similar locations together.
  \item To this end, we desired to ascertain whether the estimates of our dependence parameters were less uncertain after clustering.
  \item We can use the bootstrapping scheme described in \cite{Heffernan2004} to determine the uncertainty in our parameter estimates. \todo{Do I need to describe this scheme and how it preserves the dependence structure of the data?}
  \item As we are estimating two parameters, $\alpha$ and $\beta$, for which we do not know the true values, it may be that although our uncertainty is diminished post-clustering, the estimates may still be biased, with the parameters unidentifiable.
  \item There are also many possible combinations of $\alpha$ and $\beta$ that could give the same conditional estimates, and so the estimates may be biased in this sense as well.
  \item To avoid this, we adopt the approach of \cite{Richards2021-qm} in calculating the conditional expectation of one variable given the other at the $98\%$ marginal quantile $u$ for the conditioning variable, given by
  \begin{equation} \label{eq:boot}
    \mathbb{E}[X_1 | X_2 = u] = \hat{\alpha} u + u^{\hat{\beta}} \hat{\mu},
  \end{equation}
  where ($\hat{\alpha}, \hat{\beta}, \hat{\mu}$) are the estimated CE model parameters.
  \item We can bootstrap this conditional expectation to evaluate the reduction in uncertainty post-clustering. \todo{Tense consistency}
\end{itemize}

% Results
\begin{itemize}
  \item We take an example for which we know the clustering algorithm performs well, namely the same simulation design as in \ref{subsubsec:sim_competing_methods}, with $\rho_{\text{Gauss}} = 0.5$, and $\rho_{t} = 0.1$ and $0.9$ for the two clusters.
  \item We estimate the CE model parameters pre- and post-clustering, and generate 500 bootstrap samples for  the conditional expectation of \ref{eq:boot}.
  \item The results are plotted in figure \ref{fig:04_bootstrap}.
  \item We can see that the uncertainty in the bootstrapped estimates is vastly reduced post-clustering, as desired.
  \item The conditional expectation is higher for the higher t-copula correlation cluster, as expected. \todo{Anything else need to be said here? Be more specific with results?}

\end{itemize}

\todo{Stop clipping for x-axis title}
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.9\linewidth]{plots/sim_01e_bootstrap_box.png}
    \caption{\emph{Boxplots of 500 bootstrapped conditional expectations of one variable given the other is at the $98\%$ marginal quantile $u$, pre- and post-clustering for a simulation of 12 ``locations'', each with 1000 observations for each variable, from a mixture of Normal and Gaussian copulas, with the Gaussian correlation set to 0.5. The t-copula correlation parameters for the two clusters were set to 0.1 and 0.9, as shown on the x-axis.}}
    \label{fig:04_bootstrap}
\end{figure}


\section{Applications}
\subsection{Irish meteorological data} \label{subsec:app_irl}

\begin{itemize}
    \item 
\end{itemize}

\subsection{US urban air pollution data} \label{subsec:app_us}

\begin{itemize}
    \item 
\end{itemize}

\section{Discussion}

% Limitations
\begin{itemize}
  \item For simulations, could have looked at LRI as well as ARI.
  \item Could have looked at spatially varying clustering parameters, to more closely mimic spatial applications. 
\end{itemize}

\section*{Code availability}

\newpage
\bibliography{library}

\end{document}
